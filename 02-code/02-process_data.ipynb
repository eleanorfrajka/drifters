{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1ec2e7-c2e3-455c-8cf0-07f7212ee268",
   "metadata": {},
   "source": [
    "# SCRIPT 2\n",
    "Edit raw data and compute velocity components.\n",
    "\n",
    "Steps:\n",
    "1. ds_raw: Load an individual raw drifter dataset with GPS, temperature and other variables on non-uniform t.\n",
    "    Create tgrid_hourly from the original time data.\n",
    "2. ds_qc: Flag + remove bad GPS data\n",
    "    - Where the GPS has dropped out (= mode),\n",
    "    - Where it is outside the North Atlantic box (na_lonlim, na_latlim)\n",
    "    - Where the velocity is unrealistic (larger than 3 m/s)\n",
    "    - Where the time series of longitude and latitude (compared to a one-dimensional five-point median filter) is more than five standard deviations from the five-point median)\n",
    "     Compute the u and v velocities on the QC GPS fixes\n",
    "4. ds_lowess: Smooth the GPS (and velocities) using the Lowess filter*.  *Shane's code online doesn't seem to smooth velo with a Lowess\n",
    "5. ds_hourly: Hourly interpolate the resultant ds_lowess back onto the tgrid_hourly.  Interpolate the other parameters from ds_raw onto the hourly grid (temperature, etc)\n",
    "    > 'linear' for continuous variables\n",
    "    > 'nearest neighbour' for discrete ones\n",
    "\n",
    "6. save as netcdf file\n",
    "\n",
    "Last modified: 5 Sep 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "529f1aae-550f-4321-96f4-199417d84c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import datetime\n",
    "import gsw\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import re\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Local import\n",
    "# > Make sure SIO_wrap dir is on the same path as this script.\n",
    "\n",
    "from SIO_wrap import dir_tree, fnames\n",
    "from SIO_wrap.lowess import LatLonLocalWess\n",
    "from SIO_wrap import jlab_python as jlab\n",
    "from SIO_wrap import drifter_qc as dqc\n",
    "\n",
    "from setdir import *\n",
    "\n",
    "##################---------   LOCAL FUNCTIONS    ---------##################\n",
    "\n",
    "def latlon_extremes(var, igood):\n",
    "    \"\"\"\n",
    "    var   : xarray dataset that containg the 'flag' and lat/lon variables.\n",
    "    igood : int or float; value of the good flag\n",
    "\n",
    "    Prints the min/max of lat and lon.\n",
    "\n",
    "    <!> This function breaks if the lat/lon variable names change.\n",
    "    \"\"\"\n",
    "    # Select lat/lon where flag is good\n",
    "    bool_cond = var.flag.values==igood\n",
    "\n",
    "    lonmin = var.GPS_Longitude_deg.values[bool_cond].min().astype('str')\n",
    "    latmin = var.GPS_Latitude_deg.values[bool_cond].min().astype('str')\n",
    "    lonmax = var.GPS_Longitude_deg.values[bool_cond].max().astype('str')\n",
    "    latmax = var.GPS_Latitude_deg.values[bool_cond].max().astype('str')\n",
    "    \n",
    "    print('Lon in ('+lonmin+', '+lonmax+'), Lat in ('+latmin+', '+latmax+')')\n",
    "#    print('lat min: ', var.GPS_Latitude_deg.values[bool_cond].min())\n",
    "#    print('lat max: ', var.GPS_Latitude_deg.values[bool_cond].max())\n",
    "#    print('lon min: ', var.GPS_Longitude_deg.values[bool_cond].min())\n",
    "#    print('lon max: ', var.GPS_Longitude_deg.values[bool_cond].max())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652a35a-f76f-448e-bfca-84d9cd625b86",
   "metadata": {},
   "source": [
    "# User edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fc4be0-322d-44a6-b462-14c039710843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the output data\n",
    "data_dir = dir_tree.dir_out\n",
    "\n",
    "#--------------\n",
    "# Flag value - good=1, bad=4\n",
    "good_flagval = 1\n",
    "bad_gps_flagval = 4 \n",
    "bad_vel_flagval = 3\n",
    "\n",
    "# FLAG 1: Latitude values less than this threshold are flagged + removed?\n",
    "latbad_threshold = 0\n",
    "\n",
    "# FLAG 2: North Atlantic box; values outside this box are flagged + removed?\n",
    "# Defined from -180 to 180 (lon) and -90 to 90 (lat)\n",
    "na_lonlim = [-80, 30]\n",
    "na_latlim = [40, 80]\n",
    "\n",
    "#--------------\n",
    "# Speed threshold; values outside [-3, 3] m/s are flagged + removed?\n",
    "vel_threshold = 3 # in m/s\n",
    "\n",
    "#--------------\n",
    "# LOWESS params\n",
    "poly_order = 1\n",
    "bandwidth = 2\n",
    "\n",
    "#------------\n",
    "# Names of variables\n",
    "latname = 'GPS_Latitude_deg'\n",
    "lonname = 'GPS_Longitude_deg'\n",
    "uvelname = 'uvel'\n",
    "vvelname = 'vvel'\n",
    "\n",
    "#--------------\n",
    "# Time formats\n",
    "tstamp_strftime = '%Y%m%d'  # Filename timestamp\n",
    "timcol_strftime = '%Y-%m-%d %H:%M:%S'  # Convert text to datetime format \n",
    "\n",
    "# Reference date for computing time in seconds\n",
    "# Can use an earlier time reference if data start before 2000\n",
    "ref_time = datetime.datetime(2000, 1, 1)\n",
    "\n",
    "#--------------\n",
    "# List of variable names split between float/int types based on whether the \n",
    "# variables are continuous or discrete, respectively.\n",
    "# <!> If the names of variables change, update the lists by printing a list of \n",
    "# all the names from the raw datafile: list(xarrayDataset.keys())\n",
    "\n",
    "integ_vars = ['Drogue_cnts', 'GPS_HDOP', 'GPS_FixDelay', 'GPS_TTFF', \n",
    "              'GPS_NumSat', 'SBD_Transmit_Delay', 'SBD_Retries']\n",
    "\n",
    "float_vars = [lonname, latname, 'SST_degC',\n",
    "              'SLP_mB', 'Battery_volts', uvelname, vvelname]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bf362-b473-42bf-a392-fb1c4b0c36d2",
   "metadata": {},
   "source": [
    "# Step 1. Load raw data\n",
    "\n",
    "Check the list of PIDs and decide which ones need to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "456b02fa-e492-4568-81b8-58239a18ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid(300234066519040) - Up to date processed file exists\n",
      "pid(300234068243970) - Up to date processed file exists\n",
      "1. pid(300234068243460) - No previous raw data files.\n",
      "pid(300234068342280) - Up to date processed file exists\n",
      "pid(300234066519050) - Up to date processed file exists\n",
      "2. pid(300234068343310) - No previous raw data files.\n",
      "pid(300234068342290) - Up to date processed file exists\n",
      "3. pid(300234068343830) - No previous raw data files.\n",
      "4. pid(300234068242460) - No previous raw data files.\n",
      "pid(300234068244510) - Up to date processed file exists\n",
      "5. pid(300234066516000) - No previous raw data files.\n",
      "6. pid(300234068343840) - No previous raw data files.\n",
      "pid(300234066518050) - Up to date processed file exists\n",
      "7. pid(300234068342820) - No previous raw data files.\n",
      "8. pid(300234066517030) - No previous raw data files.\n",
      "9. pid(300234068242470) - No previous raw data files.\n",
      "10. pid(300234066516010) - No previous raw data files.\n",
      "11. pid(300234068242990) - No previous raw data files.\n",
      "12. pid(300234068242480) - No previous raw data files.\n",
      "13. pid(300234066416690) - No previous raw data files.\n",
      "14. pid(300234066516020) - No previous raw data files.\n",
      "15. pid(300234068342840) - No previous raw data files.\n",
      "16. pid(300234066517050) - No previous raw data files.\n",
      "17. pid(300234066516030) - No previous raw data files.\n",
      "18. pid(300234068343360) - No previous raw data files.\n",
      "19. pid(300234066515010) - No previous raw data files.\n",
      "20. pid(300234068345410) - No previous raw data files.\n",
      "21. pid(300234068349510) - No previous raw data files.\n",
      "22. pid(300234066516040) - No previous raw data files.\n",
      "23. pid(300234068341840) - No previous raw data files.\n",
      "24. pid(300234066516050) - No previous raw data files.\n",
      "pid(300234068243540) - Up to date processed file exists\n",
      "25. pid(300234068343380) - No previous raw data files.\n",
      "26. pid(300234066515030) - No previous raw data files.\n",
      "27. pid(300234068243550) - No previous raw data files.\n",
      "28. pid(300234066514020) - No previous raw data files.\n",
      "29. pid(300234066515050) - No previous raw data files.\n",
      "30. pid(300234068348010) - No previous raw data files.\n",
      "31. pid(300234066514030) - No previous raw data files.\n",
      "32. pid(300234066514040) - No previous raw data files.\n",
      "33. pid(300234068349560) - No previous raw data files.\n",
      "34. pid(300234068346490) - No previous raw data files.\n",
      "35. pid(300234068245630) - No previous raw data files.\n",
      "36. pid(300234066514050) - No previous raw data files.\n",
      "37. pid(300234066416770) - No previous raw data files.\n",
      "38. pid(300234066513030) - No previous raw data files.\n",
      "39. pid(300234068343430) - No previous raw data files.\n",
      "40. pid(300234068345480) - No previous raw data files.\n",
      "41. pid(300234068345990) - No previous raw data files.\n",
      "42. pid(300234066512010) - No previous raw data files.\n",
      "43. pid(300234066416780) - No previous raw data files.\n",
      "44. pid(300234068244620) - No previous raw data files.\n",
      "45. pid(300234066513040) - No previous raw data files.\n",
      "46. pid(300234068343440) - No previous raw data files.\n",
      "47. pid(300234068345490) - No previous raw data files.\n",
      "48. pid(300234068346000) - No previous raw data files.\n",
      "49. pid(300234068440210) - No previous raw data files.\n",
      "50. pid(300234066513050) - No previous raw data files.\n",
      "51. pid(300234068343450) - No previous raw data files.\n",
      "52. pid(300234068346010) - No previous raw data files.\n",
      "53. pid(300234066512030) - No previous raw data files.\n",
      "54. pid(300234066511010) - No previous raw data files.\n",
      "55. pid(300234068343970) - No previous raw data files.\n",
      "56. pid(300234066513060) - No previous raw data files.\n",
      "57. pid(300234068343460) - No previous raw data files.\n",
      "58. pid(300234066512040) - No previous raw data files.\n",
      "59. pid(300234068342440) - No previous raw data files.\n",
      "60. pid(300234066416810) - No previous raw data files.\n",
      "61. pid(300234066511020) - No previous raw data files.\n",
      "62. pid(300234066510000) - No previous raw data files.\n",
      "63. pid(300234066512050) - No previous raw data files.\n",
      "64. pid(300234068342970) - No previous raw data files.\n",
      "65. pid(300234068342460) - No previous raw data files.\n",
      "66. pid(300234066416830) - No previous raw data files.\n",
      "67. pid(300234068349630) - No previous raw data files.\n",
      "68. pid(300234066511040) - No previous raw data files.\n",
      "69. pid(300234066416840) - No previous raw data files.\n",
      "70. pid(300234068349640) - No previous raw data files.\n",
      "71. pid(300234066511050) - No previous raw data files.\n",
      "72. pid(300234068344010) - No previous raw data files.\n",
      "73. pid(300234068343500) - No previous raw data files.\n",
      "74. pid(300234066510030) - No previous raw data files.\n",
      "75. pid(300234066511060) - No previous raw data files.\n",
      "76. pid(300234068344020) - No previous raw data files.\n",
      "77. pid(300234068343510) - No previous raw data files.\n",
      "78. pid(300234068346580) - No previous raw data files.\n",
      "79. pid(300234066510040) - No previous raw data files.\n",
      "pid(300234068343000) - Up to date processed file exists\n",
      "80. pid(300234068342490) - No previous raw data files.\n",
      "81. pid(300234068440280) - No previous raw data files.\n",
      "82. pid(300234066416860) - No previous raw data files.\n",
      "83. pid(300234066510050) - No previous raw data files.\n",
      "84. pid(300234068245220) - No previous raw data files.\n",
      "85. pid(300234066416870) - No previous raw data files.\n",
      "86. pid(300234068344040) - No previous raw data files.\n",
      "87. pid(300234068243690) - No previous raw data files.\n",
      "88. pid(300234066510060) - No previous raw data files.\n",
      "89. pid(300234068346610) - No previous raw data files.\n",
      "90. pid(300234068346100) - No previous raw data files.\n",
      "pid(300234068440310) - Up to date processed file exists\n",
      "91. pid(300234068245240) - No previous raw data files.\n",
      "92. pid(300234068242680) - No previous raw data files.\n",
      "93. pid(300234068244730) - No previous raw data files.\n",
      "94. pid(300234068349690) - No previous raw data files.\n",
      "95. pid(300234068346620) - No previous raw data files.\n",
      "96. pid(300234068243710) - No previous raw data files.\n",
      "97. pid(300234068343550) - No previous raw data files.\n",
      "98. pid(300234068245250) - No previous raw data files.\n",
      "99. pid(300234068342020) - No previous raw data files.\n",
      "100. pid(300234068349700) - No previous raw data files.\n",
      "101. pid(300234068345610) - No previous raw data files.\n",
      "102. pid(300234068245260) - No previous raw data files.\n",
      "103. pid(300234068242700) - No previous raw data files.\n",
      "104. pid(300234068245270) - No previous raw data files.\n",
      "105. pid(300234068342550) - No previous raw data files.\n",
      "106. pid(300234066416410) - No previous raw data files.\n",
      "107. pid(300234068244250) - No previous raw data files.\n",
      "108. pid(300234068243230) - No previous raw data files.\n",
      "109. pid(300234068345630) - No previous raw data files.\n",
      "110. pid(300234068348190) - No previous raw data files.\n",
      "111. pid(300234066416930) - No previous raw data files.\n",
      "112. pid(300234068244260) - No previous raw data files.\n",
      "113. pid(300234068342570) - No previous raw data files.\n",
      "114. pid(300234068244270) - No previous raw data files.\n",
      "115. pid(300234068243250) - No previous raw data files.\n",
      "116. pid(300234068348210) - No previous raw data files.\n",
      "117. pid(300234068343610) - No previous raw data files.\n",
      "118. pid(300234068243260) - No previous raw data files.\n",
      "119. pid(300234068348220) - No previous raw data files.\n",
      "120. pid(300234068346690) - No previous raw data files.\n",
      "121. pid(300234068243270) - No previous raw data files.\n",
      "122. pid(300234068348230) - No previous raw data files.\n",
      "123. pid(300234068347720) - No previous raw data files.\n",
      "124. pid(300234068343630) - No previous raw data files.\n",
      "125. pid(300234068346190) - No previous raw data files.\n",
      "126. pid(300234068242260) - No previous raw data files.\n",
      "127. pid(300234068346200) - No previous raw data files.\n",
      "128. pid(300234068345690) - No previous raw data files.\n",
      "129. pid(300234068347740) - No previous raw data files.\n",
      "130. pid(300234068242270) - No previous raw data files.\n",
      "131. pid(300234068244830) - No previous raw data files.\n",
      "132. pid(300234068343650) - No previous raw data files.\n",
      "133. pid(300234068242790) - No previous raw data files.\n",
      "134. pid(300234068345710) - No previous raw data files.\n",
      "135. pid(300234068244850) - No previous raw data files.\n",
      "136. pid(300234068349810) - No previous raw data files.\n",
      "137. pid(300234068346740) - No previous raw data files.\n",
      "138. pid(300234068243830) - No previous raw data files.\n",
      "139. pid(300234068343670) - No previous raw data files.\n",
      "140. pid(300234068345210) - No previous raw data files.\n",
      "141. pid(300234068349820) - No previous raw data files.\n",
      "142. pid(300234068342660) - No previous raw data files.\n",
      "143. pid(300234068345220) - No previous raw data files.\n",
      "144. pid(300234068342150) - No previous raw data files.\n",
      "145. pid(300234068343690) - No previous raw data files.\n",
      "146. pid(300234068346250) - No previous raw data files.\n",
      "147. pid(300234068345740) - No previous raw data files.\n",
      "148. pid(300234068345230) - No previous raw data files.\n",
      "149. pid(300234068244880) - No previous raw data files.\n",
      "150. pid(300234066513300) - No previous raw data files.\n",
      "151. pid(300234068243860) - No previous raw data files.\n",
      "152. pid(300234068343190) - No previous raw data files.\n",
      "153. pid(300234068343700) - No previous raw data files.\n",
      "154. pid(300234068345240) - No previous raw data files.\n",
      "155. pid(300234068346260) - No previous raw data files.\n",
      "156. pid(300234068243870) - No previous raw data files.\n",
      "157. pid(300234068346270) - No previous raw data files.\n",
      "158. pid(300234068342690) - No previous raw data files.\n",
      "159. pid(300234068243880) - No previous raw data files.\n",
      "160. pid(300234068343210) - No previous raw data files.\n",
      "161. pid(300234068347820) - No previous raw data files.\n",
      "162. pid(300234068342190) - No previous raw data files.\n",
      "163. pid(300234068343730) - No previous raw data files.\n",
      "164. pid(300234068243890) - No previous raw data files.\n",
      "165. pid(300234068242870) - No previous raw data files.\n",
      "166. pid(300234068345270) - No previous raw data files.\n",
      "167. pid(300234068342720) - No previous raw data files.\n",
      "168. pid(300234068243910) - No previous raw data files.\n",
      "169. pid(300234068242890) - No previous raw data files.\n",
      "170. pid(300234068345290) - No previous raw data files.\n",
      "171. pid(300234068342220) - No previous raw data files.\n",
      "172. pid(300234068343250) - No previous raw data files.\n",
      "173. pid(300234068245460) - No previous raw data files.\n",
      "174. pid(300234068345300) - No previous raw data files.\n",
      "175. pid(300234066519000) - No previous raw data files.\n",
      "176. pid(300234068245470) - No previous raw data files.\n",
      "177. pid(300234068342750) - No previous raw data files.\n",
      "178. pid(300234066519010) - No previous raw data files.\n",
      "179. pid(300234068244450) - No previous raw data files.\n",
      "180. pid(300234068343270) - No previous raw data files.\n",
      "181. pid(300234068345830) - No previous raw data files.\n",
      "182. pid(300234068342250) - No previous raw data files.\n",
      "183. pid(300234068244970) - No previous raw data files.\n",
      "184. pid(300234066519020) - No previous raw data files.\n",
      "185. pid(300234068244460) - No previous raw data files.\n",
      "186. pid(300234068343280) - No previous raw data files.\n",
      "187. pid(300234068245490) - No previous raw data files.\n",
      "188. pid(300234068244980) - No previous raw data files.\n",
      "189. pid(300234068244470) - No previous raw data files.\n",
      "190. pid(300234068342270) - No previous raw data files.\n"
     ]
    }
   ],
   "source": [
    "# Check the list of Platform IDs for TERIFIC\n",
    "PID = pd.read_csv(cat_proc_path('PID_list.txt'), header='infer', index_col=0)\n",
    "\n",
    "\n",
    "# Check whether a given Platform ID has been processed and is up-to-date\n",
    "# If not, then add it to the update list\n",
    "PID_to_update = []\n",
    "counter = 0\n",
    "\n",
    "\n",
    "# Extract a list with the names of existing raw data files.\n",
    "for i in range(len(PID)):\n",
    "    pid1 = (PID[\"PID\"].values)[i].astype('str')\n",
    "    PID1 = (PID[\"PID\"].values)[i]\n",
    "\n",
    "    # Extract a list with the names of existing raw data files.\n",
    "    fname = 'pid'+str(PID1)+'_*'\n",
    "    existing_files = glob.glob(cat_interim_path(fname))\n",
    "\n",
    "    # Check whether any files exist in the intermediate directory for that PID\n",
    "    if len(existing_files) > 0:\n",
    "\n",
    "        # Extract the end date from the filename\n",
    "        existing_files = sorted(existing_files)\n",
    "        # Needs existing_files to be sorted() so the last one is latest\n",
    "        end_date = (existing_files[-1])[-11:-3] \n",
    "        t1 = datetime.datetime.strptime(end_date, '%Y%m%d')  \n",
    "\n",
    "        # Check what the latest raw file was\n",
    "        raw_files = glob.glob(cat_raw_path(fname))\n",
    "        if not len(raw_files) > 0:\n",
    "            print('No raw file for that PID!')\n",
    "\n",
    "        # Check the end date\n",
    "        raw_files = sorted(raw_files)\n",
    "        end_date_raw = (raw_files[-1])[-11:-3]\n",
    "        t_raw = datetime.datetime.strptime(end_date_raw, '%Y%m%d')\n",
    "\n",
    "        if t1==t_raw:\n",
    "            print('pid('+pid1+') - Up to date processed file exists')\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "            PID_to_update.append(PID1)\n",
    "            print(str(counter)+'. pid('+pid1+') - Ended:'+end_date\n",
    "                  +', New end:'+t_raw)\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(str(counter)+'. pid('+pid1\n",
    "              +') - No previous raw data files.')\n",
    "        PID_to_update.append(PID1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d37753de-7ea5-4863-b4b8-b5496547f526",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0b/1448f7v57h97h1dnny_s99600000gr/T/ipykernel_86601/3838590837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPID_to_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpid1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPID_to_update\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mPID1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPID_to_update\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Load a raw file to process it\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(PID_to_update)):\n",
    "    pid1 = (PID_to_update[i]).astype('str')\n",
    "    PID1 = PID_to_update[i]\n",
    "\n",
    "    ########################################################################\n",
    "    ######## Load the raw data\n",
    "    fname = 'pid'+str(PID1)+'_*'\n",
    "    raw_files = glob.glob(cat_raw_path(fname))\n",
    "    raw_files = sorted(raw_files)\n",
    "    # Find the most recent file (alphabetically, rather than by time)\n",
    "    raw_files = raw_files[-1]\n",
    "    # Open the dataset\n",
    "    ds_raw = xr.open_dataset(raw_files)\n",
    "    # Some didn't load as sorted by time.\n",
    "    ds_raw = ds_raw.sortby('time', ascending=True)\n",
    "    print(fname+' - processing')\n",
    "    \n",
    "    #--------------------------------------\n",
    "    # Total number of points \n",
    "    total_points = int(ds_raw.time.size)\n",
    "\n",
    "    # Compute a time vector in seconds\n",
    "    dtime_sec = (pd.to_datetime(ds_raw.time.values) - \n",
    "                 ref_time).total_seconds()\n",
    "    ds_raw['time_seconds'] = (\"time\", dtime_sec)\n",
    "    # Create the hourly time grid\n",
    "    tgrid_hourly, tgrid_sec = dqc.create_hourly_grid(ds_raw[\"time\"], ref_time)\n",
    "\n",
    "    # Find the modes for lat and lon\n",
    "    mode1 = stats.mode(ds_raw[latname])\n",
    "    lat_mode = mode1.mode\n",
    "    mode_count = mode1.count\n",
    "    mode1 = stats.mode(ds_raw[lonname])\n",
    "    lon_mode = mode1.mode\n",
    "\n",
    "    \n",
    "    # Create a dictionary of attributes\n",
    "    qc_attr_dict = {\"total_points_orig\": total_points}\n",
    "\n",
    "    ########################################################################\n",
    "    ######## Step 2. Create ds_qc the quality-controlled GPS fixes\n",
    "    \"\"\"\n",
    "\n",
    "    ds_qc: Flag + remove bad GPS data\n",
    "    - Where the GPS has dropped out (= mode),\n",
    "    - Where it is outside the North Atlantic box (na_lonlim, na_latlim)\n",
    "    - Where the velocity is unrealistic (larger than 3 m/s)\n",
    "    - Where the time series of longitude and latitude (compared to a one-dimensional five-point median filter) is more than five standard deviations from the five-point median)\n",
    "\n",
    "    Compute the u and v velocities on the QC GPS fixes\n",
    "    \"\"\"\n",
    "\n",
    "    ds_qc = ds_raw\n",
    "\n",
    "    # Remove variables we won't use here\n",
    "    fields_to_remove = ['Battery_volts', 'GPS_HDOP', 'GPS_FixDelay',\n",
    "                        'GPS_TTFF', 'GPS_NumSat', 'SBD_Transmit_Delay',\n",
    "                        'SBD_Retries', 'SST_degC', 'SLP_mB']\n",
    "\n",
    "    ds_qc = ds_qc.drop(fields_to_remove, errors='ignore')\n",
    "\n",
    "    # Add a field FLAG with the same time coordinate.\n",
    "    # Start by labelling all data 'good' (flag=igood)\n",
    "    ds_qc[\"flag\"] = ('time', good_flagval * np.ones(ds_qc.time.shape, \n",
    "                                                     dtype=np.int8))\n",
    "\n",
    "\n",
    "    # Where the GPS has dropped out (= mode),\n",
    "    # Based on previous investigations, there are a lot of -90 values for \n",
    "    # GPS_Latitude_deg.  \n",
    "    if mode_count > 100:\n",
    "        # Kind of arbitrary but only do this if there are a lot of mode value\n",
    "        ds_qc = dqc.flag_gps_dropout(ds_qc, latname, \n",
    "                                  lat_mode, bad_gps_flagval)\n",
    "\n",
    "        ds_qc = dqc.flag_gps_dropout(ds_qc, lonname, \n",
    "                                  lon_mode, bad_gps_flagval)\n",
    "\n",
    "    latlon_extremes(ds_qc, good_flagval)\n",
    "\n",
    "    # Print update on flagged data\n",
    "    # Need to save this to the output variable instead.\n",
    "    # Create a dictionary of attributes\n",
    "#    print(\"\\n> > Flags - stage 1 < <\\n\")\n",
    "    num_flags1 = dqc.num_ibad(ds_qc, bad_gps_flagval)\n",
    "    num_percent1 = num_flags1 / total_points\n",
    "    percentage = \"{:.1%}\".format(num_percent1)\n",
    "#    print(\"Flagged data as percentage: %s\\n\" % percentage)\n",
    "    qc_attr_dict[\"GPS_dropouts\"] = num_flags1\n",
    "\n",
    "    # Based on the region\n",
    "    ds_qc = dqc.flag_gps_region(ds_qc, latname,\n",
    "                              na_latlim, bad_gps_flagval)\n",
    "    ds_qc = dqc.flag_gps_region(ds_qc, lonname,\n",
    "                             na_lonlim, bad_gps_flagval)\n",
    "\n",
    "    latlon_extremes(ds_qc, good_flagval)\n",
    "\n",
    "\n",
    "    # Print update on flagged data\n",
    "#    print(\"\\n> > Flags - stage 1 + 2 < <\\n\")\n",
    "    num_flags2 = dqc.num_ibad(ds_qc, bad_gps_flagval)\n",
    "    num_percent2 = num_flags2 / total_points\n",
    "    percentage = \"{:.1%}\".format(num_percent2)\n",
    "#    print(\"Total flagged data as percentage: %s\\n\" % percentage)\n",
    "\n",
    "    num_flags2_diff = num_flags2 - num_flags1\n",
    "#    print(\"Flagged data at this step only: %s\\n\" % str(num_flags2_diff))\n",
    "    qc_attr_dict[\"GPS_out_of_region\"] = num_flags2_diff\n",
    "\n",
    "\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # Remove flagged data, otherwise it might affect the lowess filtering\n",
    "\n",
    "#    print(\"Removing flagged data. \\n\")\n",
    "    ds_qc = ds_qc.where(ds_qc.flag==good_flagval, drop=True) \n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "    # velocity calculation\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "    GPSlat = ds_qc[latname].to_numpy()\n",
    "    GPSlon = ds_qc[lonname].to_numpy()\n",
    "    u_orig, v_orig = jlab.latlon2uv(ds_qc['time_seconds'], GPSlat, GPSlon)\n",
    "    velmag = np.sqrt(np.square(u_orig) + np.square(v_orig))\n",
    "\n",
    "    # Plot to verify\n",
    "    if 0:\n",
    "        plt.plot(velmag)    \n",
    "\n",
    "    ds_qc[uvelname] = ('time', u_orig)\n",
    "    ds_qc[vvelname] = ('time', v_orig)\n",
    "\n",
    "    # Flag the bad stuff\n",
    "    ds_qc.flag.values[velmag > vel_threshold] = bad_vel_flagval\n",
    "    num_flags3 = dqc.num_ibad(ds_qc, bad_vel_flagval)\n",
    "    qc_attr_dict[\"velocity_exceed_threshold\"] = num_flags3\n",
    "    \n",
    "    # Need to count how many were removed here\n",
    "\n",
    "    # Removing flagged data\n",
    "    ds_qc = ds_qc.where(ds_qc.flag==good_flagval, drop=True) \n",
    "\n",
    "    if 0:\n",
    "        GPSlat = ds_qc[latname].to_numpy()\n",
    "        GPSlon = ds_qc[lonname].to_numpy()\n",
    "\n",
    "        u_orig, v_orig = jlab.latlon2uv(ds_qc['time_seconds'], GPSlat, GPSlon)\n",
    "        velmag = np.sqrt(np.square(u_orig) + np.square(v_orig))\n",
    "        plt.plot(velmag)\n",
    "\n",
    "\n",
    "    print('Flagged '+str(num_flags1)+' for GPS dropouts, '+str(num_flags2_diff)+' for region, '+str(num_flags3)+' for velo')\n",
    "\n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "    ######## Step 3. Run Lowess filter\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "    # Create entries for lowess lat/lon\n",
    "    time1 = ds_qc.time.values\n",
    "    ds_lowess = xr.Dataset(coords=dict(time=([\"time\"], time1)))\n",
    "    ds_lowess = ds_lowess.assign_attrs(ds_qc.attrs)\n",
    "\n",
    "    ds_lowess[latname] = ('time', np.ones(len(ds_qc.time)))\n",
    "    ds_lowess[lonname] = ('time', np.ones(len(ds_qc.time)))\n",
    "\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # LOWESS: Locally Weighted Scatterplot Smoother (Elipot et al 2016)\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    time_sec = ds_qc['time_seconds'].values\n",
    "    lon = ds_qc[lonname].values\n",
    "    lat = ds_qc[latname].values\n",
    "    llat, llon = LatLonLocalWess(time_sec, lon, lat, poly_order, bandwidth)\n",
    "\n",
    "    ds_lowess[latname] = ('time', llat)\n",
    "    ds_lowess[lonname] = ('time', llon)\n",
    "\n",
    "    # Cannot use the same code to Lowess filter the velocities since it treats \n",
    "    # latitude and longitude differently.\n",
    "    ds_lowess\n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "    ######## Step 4. Interpolate hourly\n",
    "\n",
    "    # Revised from Oana's method (using arrays) to use dataset in xarray.\n",
    "\n",
    "\n",
    "    # Empty lists for storing the hourly data and new column names\n",
    "    hourly_arr = []\n",
    "    arr_names = []\n",
    "\n",
    "\n",
    "    total_points_qc = int(ds_raw.time.size)\n",
    "    qc_attr_dict[\"qc_points\"] = total_points_qc\n",
    "\n",
    "\n",
    "    # Create the new hourly xarray dataset\n",
    "    ds_hourly = xr.Dataset(coords=dict(time=([\"time\"], tgrid_hourly)))\n",
    "    ds_hourly = ds_hourly.assign_attrs(ds_lowess.attrs)\n",
    "    ds_hourly\n",
    "    time_sec = ds_qc.time_seconds\n",
    "    time_sec_raw = ds_raw.time_seconds\n",
    "\n",
    "    # Get the variable names\n",
    "    varnames_list = list(ds_raw.keys())\n",
    "    varnames_qc_list = list(ds_qc.keys())\n",
    "    varnames_lowess_list = list(ds_lowess.keys())\n",
    "\n",
    "    # Create temporary array to store hourly interp data\n",
    "    c_len = len(tgrid_hourly)\n",
    "    arr = np.ones((c_len,))\n",
    "\n",
    "    # Iterate through the variable names and set the interpolation method \n",
    "    # based on the dtype of the data\n",
    "    k = 0\n",
    "    for varname in set(varnames_list + varnames_qc_list):\n",
    "        if (varname in varnames_lowess_list):\n",
    "            # Lowess filtered variables (GPS lat and lon)\n",
    "            if varname in integ_vars:\n",
    "                interp_method = 'nearest'\n",
    "            elif varname in float_vars:\n",
    "                interp_method = 'linear'\n",
    "\n",
    "            var = ds_lowess[varname].values\n",
    "            fc_interp = interp1d(time_sec, var, interp_method,\n",
    "                                 fill_value='extrapolate')\n",
    "            arr = fc_interp(tgrid_sec)\n",
    "\n",
    "            ds_hourly[varname] = ('time', arr)\n",
    "            #            print('Using lowess for '+varname)\n",
    "\n",
    "        elif (varname in varnames_qc_list):\n",
    "            # QC drifter variables (velocity)\n",
    "            if varname in integ_vars:\n",
    "                interp_method = 'nearest'\n",
    "            elif varname in float_vars:\n",
    "                interp_method = 'linear'\n",
    "\n",
    "            var = ds_qc[varname].values\n",
    "            fc_interp = interp1d(time_sec, var, interp_method,\n",
    "                                 fill_value='extrapolate')\n",
    "            arr = fc_interp(tgrid_sec)\n",
    "\n",
    "            ds_hourly[varname] = ('time', arr)\n",
    "             #           print('Using qc for '+varname)\n",
    "\n",
    "\n",
    "        elif (varname in integ_vars) or (varname in float_vars):\n",
    "            # Raw drifter variables\n",
    "            if varname in integ_vars:\n",
    "                interp_method = 'nearest'\n",
    "            elif varname in float_vars:\n",
    "                interp_method = 'linear'\n",
    "\n",
    "            var = ds_raw[varname].values\n",
    "            fc_interp = interp1d(time_sec_raw, var, interp_method,\n",
    "                                 fill_value='extrapolate')\n",
    "            arr = fc_interp(tgrid_sec)\n",
    "\n",
    "            ds_hourly[varname] = ('time', arr)\n",
    "        else:\n",
    "            print(\"Variables not interpolated hourly: < %s >\\n\" % varname)\n",
    "\n",
    "\n",
    "    # attributes for Dataset\n",
    "    #ds_hourly.time_seconds.attrs[\"units\"] = \"seconds since %s\" % str(ref_time.strftime(timcol_strftime))\n",
    "    ds_hourly[lonname].attrs[\"long_name\"] = \"longitude_lowess\"\n",
    "    ds_hourly[latname].attrs[\"long_name\"] = \"latitude_lowess\"\n",
    "    ds_hourly[uvelname].attrs[\"units\"] = \"m/s\"\n",
    "    ds_hourly[vvelname].attrs[\"units\"] = \"m/s\"\n",
    "    ds_hourly[uvelname].attrs[\"long_name\"] = \"zonal_velocity\"\n",
    "    ds_hourly[vvelname].attrs[\"long_name\"] = \"meridional_velocity\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Update attributes\n",
    "    dstr = datetime.datetime.today()\n",
    "    dstr = dstr.replace(hour=0, minute=0, second=0, \n",
    "                        microsecond=0).strftime('%Y-%m-%d')\n",
    "\n",
    "    maxtime = tgrid_hourly.max()\n",
    "    maxtimestr = pd.to_datetime(maxtime).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    procstr = f'Flagged GPS out of LabSea, ran Lowess filter with poly_order({poly_order}) and bandwidth({bandwidth})'\n",
    "\n",
    "    # Create a dictionary of attributes\n",
    "    attr_dict = {\"Platform_ID\": PID1,\n",
    "                 \"End Time\": maxtimestr,\n",
    "                 \"Date created\": dstr,\n",
    "                 \"Processing\": procstr,\n",
    "                }\n",
    "\n",
    "\n",
    "    ds_hourly = ds_hourly.assign_attrs(attr_dict)\n",
    "    ds_hourly = ds_hourly.assign_attrs(qc_attr_dict)\n",
    "\n",
    "    # Output filename (will be the same as raw, but in 02-intermediate/)\n",
    "    data_fname = raw_files[-30:-3]+'.nc'\n",
    "    data_fpath = cat_interim_path(data_fname)\n",
    "\n",
    "    ds_hourly.to_netcdf(data_fpath)\n",
    "\n",
    "    ds_hourly\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time-start_time\n",
    "    print('Done in '+str(int(elapsed_time))+' seconds')\n",
    "    \n",
    "    PID_to_update.remove(PID1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82f4ae3f-203d-429c-9401-d5f3fd51725c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300234068243460"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18df33-1e56-4301-a9d5-2375ff2d06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "\n",
    "    # ~ ~ print update ~ ~ \n",
    "    if len(existing_files) > 0:\n",
    "        print(\"Existing raw data files: \\n%s\\n\" % existing_files)\n",
    "    else:\n",
    "        sys.exit(\"No previous raw data files.\\n\")\n",
    "\n",
    "    # ~ ~ filenaming convention ~ ~\n",
    "    # If there are multiple files with raw data (i.e. non-updated datasets), select \n",
    "    # the latest one updated.\n",
    "    # The file names are distinguished by the timestamp appended to the filename \n",
    "    # and has <tstamp_strftime> format (see 'user edits' section).\n",
    "    # The data are cropped such that the last day is fully sampled (spans 0h-23h).\n",
    "    # The timestamp in the filename is the latest downloaded fully sampled day.\n",
    "\n",
    "    # Extract the timestamp part of the filename(s) in a list\n",
    "    tstamp = [date for file in existing_files \n",
    "                for date in re.findall(\"(\\d{8})\", file)]\n",
    "\n",
    "    # Convert to datetime and pick the most recent timestamp\n",
    "    tstamp_date = pd.to_datetime(tstamp, format=tstamp_strftime)\n",
    "    fname_timestamp = tstamp[tstamp_date.argmax()]\n",
    "\n",
    "    # Load the raw file with the latest timestamp\n",
    "    ds_fname = f\"{fnames.fname_rawdata}{fname_timestamp}.nc\"\n",
    "    ds_fpath = os.path.join(data_dir, ds_fname)\n",
    "\n",
    "    print(\"Opening file: %s\\n\" % ds_fpath)\n",
    "    ds_raw = xr.open_dataset(ds_fpath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
